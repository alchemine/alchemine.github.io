---
title: Graph Neural Network
tags: MachineLearning_Base
---

# Remarks
[[논문 리뷰] Graph Neural Networks (GCN, GraphSAGE, GAT) - 김보민](https://www.youtube.com/watch?v=yY-DpulpUwk)의 내용을 정리한 글입니다.

<!--more-->
---


# 1. Introduction to Graph Neural Networks
## 1.1 Notation
Given a graph $G$:
- $V$: set of vertices
- $A$: binary adjacency matrix
- $X \in R^{m \times \mid V \mid}$: a matrix of node features

## 1.2 Graph tasks
1. Node classification \
   Categorize users/items
2. Link prediction \
   Grpah completion
3. Graph classification \
   Molecule property prediction
4. Clustering \
   Social circle detection
5. Graph generation \
   Drug discovery


# 2. Graph Convolution Networks (GCN, ICLR 2017)
[Kipf, Thomas N., and Max Welling. "Semi-supervised classification with graph convolutional networks." arXiv preprint arXiv:1609.02907 (2016).](https://arxiv.org/abs/1609.02907)

## 2.1 Learning node embeddings: iterative method
### 2.1.1 GCN layer
$H^{(l+1)} = \sigma(\tilde D^{-1/2} \tilde A \tilde D^{-1/2} \ H^{(l)} W^{(l)})$ \
$H^{(l+1)} = \sigma(\quad \quad \ \  \hat A \ \ \ \quad\quad H^{(l)} W^{(l)})$ \
$\sigma: \text{ReLU}$


1. $H^{(l)} \in R^{N \times D}$: matrix of activations in the $l$-th layer
2. $\tilde A = A + I_N$: adjacency matrix + self-connections \
   $\tilde D_{ii} = \sum_j \tilde A_{ij}$: diagonal matrix
3. $\hat A = \tilde D^{-1/2} \tilde A \tilde D^{-1/2}$: normalized adjacency  matrix (preprocessing) \
   Smaller degree$_{i}$ → bigger $A'_{ii}$
4. $H^{(0)} = X$
5. $\hat A H^{(l)}$: **aggregation**(linear combination of node feature[^1]) \
   $i$-th row: weighted sum of **adjacent node features** of $i$-th node

#### ex) 2-layer GCN (classification)
1. Preprocessing \
$\hat A = \tilde D^{-1/2} \tilde A \tilde D^{-1/2}$
1. Forward \
$Z_1 \ \ = \quad \quad \quad \quad \text{ReLU} (\hat A X W^{(0)})$ \
$Z_{out} = \text{softmax}(\text{ReLU} (\hat A Z_1 W^{(1)}))$
1. Loss function(CEE) \
$L = \sum_l \sum_f y_{lf} \ln Z_{lf}$


## 2.2 Deep Graph Library(DGL)
[DEEP GRAPH LIBRARY: Easy Deep Learning on Graphs](https://www.dgl.ai)

```python
from torch import nn
import torch.nn.functional as F
from dgl.nn import GraphConv


class GCN(nn.Module):
    def __init__(self, in_feats, h_feats, num_classes):
        super(GCN, self).__init__()
        self.conv1 = GraphConv(in_feats, h_feats)
        self.conv2 = GraphConv(h_feats, num_classes)

    def forward(self, g, in_feat):
        h = self.conv1(g, in_feat)
        h = F.relu(h)
        h = self.conv2(g, h)
        return h
```


# 3. GraphSAGE (NeurIPS 2017)
[Hamilton, Will, Zhitao Ying, and Jure Leskovec. "Inductive representation learning on large graphs." Advances in neural information processing systems 30 (2017).](https://proceedings.neurips.cc/paper_files/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html)

## 3.1 GCN → GraphSAGE
- **GraphSAGE** \
Generating embeddings by **SAmpling and AGgregating** features from a node's local neighborhood

![Alt text](/images/2023-09-08-gnn-1.png)

- `AGGREGATE`
  - GraphSAGE-GCN: $\text{AGGREGATE}$ = weighted sum
  - GraphSAGE-mean: $\text{AGGREGATE}$ = mean
  - GraphSAGE-LSTM: $\text{AGGREGATE}$ = LSTM (high cost but not good)
  - GraphSAGE-pool: $\text{AGGREGATE}$ = pool


## 3.2 Deep Graph Library(DGL)
```python
from torch import nn
import torch.nn.functional as F
from dgl.nn import SAGEConv


class GCN(nn.Module):
    def __init__(self, in_feats, h_feats, num_classes):
        super(GCN, self).__init__()
        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')
        self.conv2 = SAGEConv(h_feats, num_classes, 'mean')

    def forward(self, g, in_feat):
        h = self.conv1(g, in_feat)
        h = F.relu(h)
        h = self.conv2(g, h)
        return h
```


b # 4. Graph Attention Networks (GAT, ICLR 2018)
[Veličković, Petar, et al. "Graph attention networks." arXiv preprint arXiv:1710.10903 (2017).](https://arxiv.org/abs/1710.10903)

## 4.1 GNN applied Attention
![Alt text](/images/2023-09-08-gnn-2.png)

1. **Attention components**
	1. Query: $h_j \in R^{F}$
	2. Key, value: $h_i \in R^{F}$
	3. Masked if not neighbors of $j$'th node
	4. Projection matrix: $W \in R^{F' \times F}$
2. **Attention algorithm**
	1. Transform input features($F$) into high-level features($F'$)
		1. Query: $h_j \rarr Wh_j$
		2. Key: $h_i \rarr Wh_i$
	2. Compute attention coefficients
		- Attention mechanism $a: R^{2F'} \rarr R$
			- Single-layer FC with LeakyReLU
		 	- $2F'$: concatenated query($F'$) and key($F'$)
		- Compute attention coefficient \
		$e_{ij} = a(Wh_i, Wh_j)$
	3. Normalize attention coefficient \
	$\alpha_{ij} = \text{softmax}_j(e_{ij})$
	4. Compute a linear combination of the features \
	$h'_i = \sigma(\sum_j \alpha_j Wh_j)$
	5. Employ multi-head attention to stabilize the learning process \
	$h'^k_i = \sigma(\sum_j \alpha^k_j W^kh_j)$ \
	$h'_i = \text{concat}_k(h'^k_i)$

		- On final(prediction) layer \
		$h'_i = \sigma(\text{mean}_k(\sum_j \alpha^k_j W^kh_j))$
			- Employ **averaging** instead of concatenation
			- Delay applying nonlinarity

## 4.2 Deep Graph Library(DGL)
```python
from torch import nn
import torch.nn.functional as F
from dgl.nn import GATConv
 

class GAT(nn.Module):
    def __init__(self, insize, hid_size, out_size, heads):
        super().__init__()
        self.conv1 = GATConv(in_size, hid_size, heads[0], feat_drop=0.6, attn_drop=0.6, activation=F.elu)
        self.conv2 = GATConv(hid_size*heads[0], out_size, heads[1], feat_drop=0.6, attn_drop=0.6, activation=None)

    def forward(self, g, inputs):
        h = self.conv1(g, inputs)
		h = h.flatten(1)
        h = self.conv2(g, h)
		h = h.mean(1)
        return h
```


---
[^1]: [Elementary row operation](https://alchemine.github.io/2019/07/17/ero.html#2-elementary-row-operation)

---
title: BERT - Bidirectional Encoder Representations from Transformer
tags: Paper
---

# Remarks
[고려대학교 산업경영공학부 강필성 교수님의 영상](https://www.youtube.com/watch?v=IwtexRHoWG0) 등을 참고

<!--more-->
---

### 1. Transformer의 encoder 구조만을 사용
- BERT의 목표가 **언어 모델링(Language Modeling, LM)**이기 때문
- Application은 위의 **한 개의 dense layer(+GELU+Norm)만을 추가**하고 pre-training과 fine-tuning 하여 사용


### 2. Pre-trained model
기본적으로 2가지 task를 동시에 학습시키는 pre-training을 수행
- 입력: `[CLS] my dog is cute [SEP] he likes playing [SEP]`

1. Masked Language Modeling(MLM) \
    ![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC9.PNG)[^1]
    - 출력: 마스크/변경/그대로 처리된 토큰의 원래 단어 (multi-class classification) \
    e.g. `am`, `he`, `play`
2. Next Sentence Prediction(NSP) \
    ![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC10.PNG)
    - 출력: 두 번째 문장이 첫 번째 문자 다음에 오는 문장인지 여부 (binary classification) \
    e.g. `True`


### 3. Embeddings
Input sequence는 여러 embedding을 취한 합이 BERT로 들어가게 된다. \
즉, token embedding + position embedding + segment embedding (512-dim)

1. Token(WordPiece) Embedding \
Token에 대한 representation vector로 변환 (단어 집합의 크기: 30,522개, reprentation vector: **768차원**)
2. Position Embedding \
각 token의 순서 정보를 보존 (문장의 최대 길이: 512개)
3. Segment Embedding \
두 개의 문장을 구분 (문장의 최대 개수: 2개)

### 4. Applications
Task 종류에 따라, `Class Label` 만을 사용할지 나머지 token들을 사용할 지 결정

![Alt text](/images/2023-04-12-bert-1.png)

---

[^1]: [https://wikidocs.net/115055](https://wikidocs.net/115055)
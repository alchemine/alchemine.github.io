---
title: optimizer.zero_grad()
tags: PyTorch
aside:
  toc: true
---

<!--more-->


{% highlight python linenos %}
import torch

W = torch.tensor(2.0, requires_grad=True)

for epoch in range(5):
    z = 2*W
    z.backward()
    print("dz/dW =", W.grad.item())
{% endhighlight %}

    dz/dW = 2.0
    dz/dW = 4.0
    dz/dW = 6.0
    dz/dW = 8.0
    dz/dW = 10.0


`optimizer.zero_grad()`는 `loss.backward()`에서 계산된 $\frac{\partial L}{\partial W}$의 값을 0으로 초기화하는 역할을 한다. \
이를 직접 초기화하지 않은 경우, gradient가 누적되는 것을 확인할 수 있다.

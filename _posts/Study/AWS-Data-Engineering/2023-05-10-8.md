---
title: 8. Spark
tags: Study_AWS-Data-Engineering
---

<!--more-->

# 1. 분산처리 문제
Spark: 분산처리를 병렬처리 수준으로 문제를 추상화 \
분산처리에서는 신경써야할 문제들이 많음

1. 부분 실패: 일부의 노드가 프로그램과 무관한 이유로 실패 \
Spark는 RDD의 불변한 성질을 이용하여 노드의 상태를 되돌렸다가 다시 연산하는 처리과정을 자동으로 수행
2. **속도: 많은 네트워크 통신을 필요로 하는 경우 속도 저하** \
Spark의 뒷단에서 어떻게 돌아갈지 상상하며 코딩해야함


# 2. Structured Data and RDD
Action이 되기 전까지는 연산이 되지 않는 RDD
- Single Value RDD
- Key-Value RDD: (key, value) 쌍을 갖는 RDD
    ```python
    pairs = rdd.map(lambda x: (x, 1))
    count = pairs.reduceByKey(lambda a, b,: a+b)  # reduction
    ```
    - Key를 바꾸지 않는 경우, `map()` 대신 Spark 내부에서 파티션을 유지할 수 있는 `mapValues()`를 사용


# 3. Transformations & Actions
1. Transformations
    - 결과값으로 새로운 RDD를 반환
    - Lazy Execution
2. Actions
    - 결과값을 연산하여 출력하거나 저장
    - Eager Execution

3. Example
    ```python
    from pyspark import SparkConf, SparkContext

    conf = SparkConf().setMaster('local').setAppName("transformations_actions")
    sc = SparkContext(conf=conf)

    foods = sc.parallelize([1, 2, 3, 4, 5])
    foods.countByValue()
    ```

# 4. Cache & Persist
1. Cache
   - 디폴트 Storage Level 사용
   - RDD: MEMORY_ONLY
   - DF: MEMORY_AND_DISK
2. Persist
   - Storage Level을 사용자가 지정 가능


# 5. Cluster Topology
Spark의 작업이 뒷단에서 어떻게 돌아갈지 상상하며 효율적으로 프로그래밍하는 것이 중요

**Master - Worker Topology**
![](https://wikidocs.net/images/page/26630/cluster-overview.png)
